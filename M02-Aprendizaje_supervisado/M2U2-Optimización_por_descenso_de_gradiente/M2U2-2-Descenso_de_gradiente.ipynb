{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JavaGenbu/Machine-Learning/blob/main/M02-Aprendizaje_supervisado/M2U2-Optimizaci%C3%B3n_por_descenso_de_gradiente/M2U2-2-Descenso_de_gradiente.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a483fea-5052-486a-a650-5801d6883aa1",
      "metadata": {
        "id": "6a483fea-5052-486a-a650-5801d6883aa1"
      },
      "source": [
        "# Regresión lineal multivariable: Descenso de gradiente\n",
        "M2U2 - Ejercicio 2\n",
        "\n",
        "## ¿Qué vamos a hacer?\n",
        "- Implementar la optimización de la función de coste por gradient descent, o lo que es lo mismo, entrenar el modelo\n",
        "\n",
        "Recuerda seguir las instrucciones para las entregas de prácticas indicadas en [Instrucciones entregas](https://github.com/Tokio-School/Machine-Learning/blob/main/Instrucciones%20entregas.md).\n",
        "\n",
        "## Instrucciones\n",
        "\n",
        "Este ejercicio es una continuación del ejercicio anterior \"Función de coste\", por lo que debes basarte en el mismo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd43af36-a639-4381-8424-72241955e713",
      "metadata": {
        "id": "fd43af36-a639-4381-8424-72241955e713"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ed96c4-774a-4e0b-88a2-9c23c7f9181b",
      "metadata": {
        "id": "e0ed96c4-774a-4e0b-88a2-9c23c7f9181b"
      },
      "source": [
        "## Tarea 1: Implementar la función de coste para regresión lineal multivariable\n",
        "\n",
        "En esta tarea, debes copiar la celda correspondiente del ejercicio anterior, trayendo tu código para implementar la función de coste vectorizada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3300d424-c3ba-47d5-9254-65716fbd1a6b",
      "metadata": {
        "id": "3300d424-c3ba-47d5-9254-65716fbd1a6b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cost_function(x, y, theta):\n",
        "    \"\"\" Computa la función de coste para el dataset y coeficientes considerados.\n",
        "\n",
        "    Argumentos posicionales:\n",
        "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
        "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1\n",
        "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
        "\n",
        "    Devuelve:\n",
        "    j -- float con el coste para dicho array theta\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "\n",
        "    h = x.dot(theta.T)\n",
        "    j = (1/(2*m)) * np.sum(np.square(h - y))\n",
        "\n",
        "    return j"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b09977c-81e4-44d3-babf-e4a6ee0a9fb7",
      "metadata": {
        "tags": [],
        "id": "7b09977c-81e4-44d3-babf-e4a6ee0a9fb7"
      },
      "source": [
        "## Tarea 2: Implementar la optimización de dicha función de coste por gradient descent\n",
        "\n",
        "Ahora vamos a resolver la optimización de dicha función de coste para entrenar el modelo, mediante el método de gradient descent de forma vectorizada. El modelo se considerará entrenado cuando su función de coste haya alcanzado un valor mínimo y estable.\n",
        "\n",
        "$$Y = h_\\Theta(X) = X \\times \\Theta^T$$\n",
        "\n",
        "$$J_\\theta = \\frac{1}{2m} \\sum_{i = 0}^{m} (h_\\theta(x^i) - y^i)^2$$\n",
        "\n",
        "$$\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i = 0}^{m}{(h_\\theta(x^i) - y^i) x_j^i}$$\n",
        "\n",
        "Para ello, de nuevo, rellena la plantilla de código de la siguiente celda.\n",
        "\n",
        "Consejos:\n",
        "- Si lo prefieres, puedes implementar primero la función con bucles e iteraciones y por último de forma vectorizada\n",
        "- Recuerda las dimensiones de cada vector/matriz\n",
        "- De nuevo, anota las operaciones por orden paso a paso en una hoja o celda auxiliar\n",
        "- En cada paso, anota las dimensiones de su resultado, que también puedes comprobar en tu código\n",
        "- Usa numpy.matmul() como multiplicación de matrices\n",
        "- Al inicio de cada iteración de entrenamiento, debes copiar toda $\\Theta$, puesto que vas a iterar actualizando cada uno de sus valores basándote en el vector completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fa0980f-0796-4f79-aed6-d02f9e95d495",
      "metadata": {
        "id": "9fa0980f-0796-4f79-aed6-d02f9e95d495"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(x, y, theta, alpha, e=1e-4, iter_=1e3):\n",
        "    \"\"\" Entrena el modelo optimizando su función de coste por gradient descent\n",
        "    \n",
        "    Argumentos posicionales:\n",
        "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
        "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1\n",
        "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
        "    alpha -- float, ratio de entrenamiento\n",
        "    \n",
        "    Argumentos nombrados (keyword):\n",
        "    e -- float, diferencia mínima entre iteraciones para declarar que el entrenamiento ha convergido finalmente\n",
        "    iter_ -- int/float, nº de iteraciones\n",
        "    \n",
        "    Devuelve:\n",
        "    j_hist -- list/array con la evolución de la función de coste durante el entrenamiento\n",
        "    theta -- array de Numpy con el valor de theta en la última iteración\n",
        "    \"\"\"\n",
        "    iter_ = int(iter_)\n",
        "    j_hist = np.zeros(iter_)\n",
        "    \n",
        "    m, n = x.shape\n",
        "    \n",
        "    for k in range(iter_):\n",
        "        theta_iter = np.copy(theta)\n",
        "        \n",
        "        for j in range(n):\n",
        "            h = np.dot(x, theta_iter)\n",
        "            theta_iter[j] = theta[j] - alpha * np.sum((h - y) * x[:, j]) / m\n",
        "        \n",
        "        theta = theta_iter\n",
        "        \n",
        "        cost = cost_function(x, y, theta)\n",
        "        j_hist[k] = cost\n",
        "        \n",
        "        if k > 0 and np.abs(j_hist[k] - j_hist[k - 1]) < e:\n",
        "            print('Converge en la iteración nº: ', k)\n",
        "            break\n",
        "    else:\n",
        "        print('Nº máx. de iteraciones alcanzado')\n",
        "        \n",
        "    return j_hist[:k], theta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b469bd3-80f2-4cbe-baa5-6a10c06f4ca1",
      "metadata": {
        "id": "8b469bd3-80f2-4cbe-baa5-6a10c06f4ca1"
      },
      "source": [
        "## Tarea 3: Comprobar la implementación del gradient descent\n",
        "\n",
        "Para comprobar tu implementación, de nuevo, utiliza la misma celda variando sus parámetros varias veces, representando gráficamente la evolución de la función de coste y viendo cómo su valor va acercándose a 0.\n",
        "\n",
        "En cada caso, comprueba que la $\\Theta$ inicial y final son muy similares en los siguientes escenarios:\n",
        "1. Genera varios datasets sintéticos, comprobando cada uno\n",
        "1. Modifica el nº de ejemplos y características, m y n\n",
        "1. Modifica el parámetro de error, lo que puede hacer que la $\\Theta$ inicial y final no concuerden del todo, y a mayor error más diferencia puede haber\n",
        "1. Comprueba los hiper-parámetros del nº máx. de iteraciones o el ratio de entrenamiento $\\alpha$, que hará que el modelo tarde más o menos en entrenarse, dentro de unos valores mínimos y máximos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3609719f-aab2-4ea3-94c5-6c05c72deabe",
      "metadata": {
        "id": "3609719f-aab2-4ea3-94c5-6c05c72deabe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Genera el dataset sintético\n",
        "m = 100\n",
        "n = 5\n",
        "e = 0.5\n",
        "\n",
        "X = np.random.randn(m, n)\n",
        "Theta_verd = np.random.randn(1, n)\n",
        "Y = np.dot(X, Theta_verd.T) + e*np.random.randn(m, 1)\n",
        "\n",
        "# Normaliza las variables independientes\n",
        "X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "# Inicializa los parámetros del modelo\n",
        "theta = np.zeros((1, n))\n",
        "\n",
        "# Define los hiperparámetros del modelo\n",
        "alpha = 0.01\n",
        "e = 1e-5\n",
        "iter_ = 1000\n",
        "\n",
        "# Entrena el modelo por gradient descent\n",
        "j_hist, theta = gradient_descent(X_norm, Y, theta, alpha, e, iter_)\n",
        "\n",
        "# Compara la evolución de la función de coste con la solución analítica\n",
        "plt.plot(j_hist)\n",
        "plt.xlabel('Iteraciones')\n",
        "plt.ylabel('Coste')\n",
        "plt.title('Evolución de la función de coste durante el entrenamiento')\n",
        "plt.show()\n",
        "\n",
        "# Compara el valor final de los coeficientes del modelo con la solución analítica\n",
        "print('Valor final de los coeficientes del modelo:')\n",
        "print(theta)\n",
        "print('Solución analítica:')\n",
        "print(Theta_verd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c92d7f8c-ef7e-4358-b027-32f0fac388ed",
      "metadata": {
        "id": "c92d7f8c-ef7e-4358-b027-32f0fac388ed"
      },
      "outputs": [],
      "source": [
        "# Utiliza una theta iniciada aleatoriamente o la Theta_verd, en función del escenario a comprobar\n",
        "theta_ini = np.random.rand(n)\n",
        "\n",
        "print('Theta inicial:')\n",
        "print(theta_ini)\n",
        "\n",
        "alpha = 1e-1\n",
        "e = 1e-3\n",
        "iter_ = 1e3\n",
        "\n",
        "print('Hiper-arámetros usados:')\n",
        "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
        "\n",
        "t = time.time()\n",
        "j_hist, theta_final = gradient_descent(X, Y, theta_ini, alpha, e, iter_)\n",
        "\n",
        "print('Tiempo de entrenamiento (s):', time.time() - t)\n",
        "\n",
        "print('\\nÚltimos 10 valores de la función de coste')\n",
        "print(j_hist[-10:])\n",
        "print('\\Coste final:')\n",
        "print(j_hist[-1])\n",
        "print('\\nTheta final:')\n",
        "print(theta_final)\n",
        "\n",
        "print('Valores verdaderos de Theta y diferencia con valores entrenados:')\n",
        "print(Theta_verd)\n",
        "print(theta_final - Theta_verd)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a26243d6-8737-417c-acbb-1ae426dd05d0",
      "metadata": {
        "id": "a26243d6-8737-417c-acbb-1ae426dd05d0"
      },
      "source": [
        "Representa gráficamente el histórico de la función de coste para comprobar tu implementación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27fdecba-cf90-4d35-99b7-5da9cc2b00a5",
      "metadata": {
        "id": "27fdecba-cf90-4d35-99b7-5da9cc2b00a5"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "\n",
        "plt.title('Función de coste')\n",
        "plt.xlabel('nº iteraciones')\n",
        "plt.ylabel('coste')\n",
        "\n",
        "plt.plot(j_hist)\n",
        "\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa3a720-c473-4e29-b3c7-0bda5bea5bbb",
      "metadata": {
        "id": "caa3a720-c473-4e29-b3c7-0bda5bea5bbb"
      },
      "source": [
        "Para comprobar completamente la implementación de dichas funciones, modifica el dataset sintético original para comprobar que la función de coste y el entrenamiento por gradient descent siguen funcionando correctamente.\n",
        "\n",
        "P. ej., modifica el nº de ejemplos y el nº de características.\n",
        "\n",
        "También añádele de nuevo un término de error a la Y. En este caso, puede que la Theta inicial y la final no concuerden del todo, ya que hemos introducido error o \"ruido\" en el dataset de entrenamiento.\n",
        "\n",
        "Por último, comprueba todos los hiper-parámetros de tu implementación. Utiliza varios valores de alpha, e, nº de iteraciones, etc., y comprueba que los resultados son los esperados."
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m87",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}